{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Common \n",
    "import tensorflow\n",
    "import numpy as np\n",
    "\n",
    "# Model Definition \n",
    "import os\n",
    "from keras import optimizers\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Activation, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "\n",
    "# Training\n",
    "from keras.callbacks import TensorBoard, ReduceLROnPlateau, ModelCheckpoint\n",
    "from time import strftime\n",
    "\n",
    "# Visualizations \n",
    "from keras.utils import plot_model\n",
    "\n",
    "# Image pre-processing\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 224\n",
    "color_mode = 'rgb'     # \"grayscale\" or \"rgb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Train and Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainAndTestSet(image_size, batch_size, color_mode):\n",
    "    train_data_dir = './datasets/equal_data/gender/train/'\n",
    "    valid_data_dir = './datasets/equal_data/gender/valid/'\n",
    "    \n",
    "    train_datagen = ImageDataGenerator(rescale=1.0/255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        color_mode=color_mode,\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical'\n",
    "    )\n",
    "    \n",
    "    valid_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "    valid_generator = valid_datagen.flow_from_directory(\n",
    "        valid_data_dir,\n",
    "        color_mode=color_mode,\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical'\n",
    "    )\n",
    "    \n",
    "    num_samples=train_generator.samples\n",
    "    num_classes=train_generator.num_classes\n",
    "    num_men = sum(train_generator.classes == 1)\n",
    "    num_woman = sum(train_generator.classes ==0)\n",
    "    print(\"num woman:\", num_woman)\n",
    "    print(\"num men:\", num_men)\n",
    "    num_valid = valid_generator.samples\n",
    "    \n",
    "    return train_generator, valid_generator, num_valid, num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel_vgg11(image_shape, num_classes, name=\"vgg11\"):\n",
    "    ## VGG16 architecture: https://arxiv.org/pdf/1409.1556.pdf\n",
    "    ## Also may be able to use VGG-Face pre-trained weights. Not sure if the VGG-Face\n",
    "    ## architecture is the same as the VGG-16. \n",
    "    ## Note: if the architectures don't match, they probably only differ by a small \n",
    "    ## amount, so we can probably create a separate VGG-Face model based on our VGG16\n",
    "    ## and then use the weights from Oxford: http://www.robots.ox.ac.uk/~vgg/software/vgg_face/ \n",
    "    \n",
    "    # This implementation is based on Configuration D from page 3 of 1409.1556.pdf, so 16 weight layers total: \n",
    "    \n",
    "    # Input (image): \n",
    "    # Note, I read somewhere that for tensorflow the order matters for performance, \n",
    "    # so check if this should be (1, img_size, img_size) instead? \n",
    "    image_input = Input(shape=image_shape, name=\"image_input\")\n",
    "    \n",
    "    # Note about pre-trained weights: \n",
    "    # We have to do some potentially different pre-processing depending on which \n",
    "    # pre-trained weights we use (if we use any). For example per-pixel mean-centering: \n",
    "    # https://gist.github.com/ksimonyan/211839e770f7b538e2d8#file-readme-md \n",
    "    \n",
    "    # Group 1: \n",
    "    x = Conv2D(filters=64, kernel_size=(3, 3), strides=(1,1), activation=\"relu\", name=\"conv2d_g1_01\")(image_input)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name=\"maxpool_g1\")(x)\n",
    "    \n",
    "    # Group 2: \n",
    "    x = Conv2D(filters=128, kernel_size=(3, 3), strides=(1,1), activation=\"relu\", name=\"conv2d_g2_01\")(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name=\"maxpool_g2\")(x)\n",
    "        \n",
    "    # Group 3: \n",
    "    x = Conv2D(filters=256, kernel_size=(3, 3), strides=(1,1), activation=\"relu\", name=\"conv2d_g3_01\")(x)\n",
    "    x = Conv2D(filters=256, kernel_size=(3, 3), strides=(1,1), activation=\"relu\", name=\"conv2d_g3_02\")(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name=\"maxpool_g3\")(x)\n",
    "        \n",
    "    # Group 4: \n",
    "    x = Conv2D(filters=512, kernel_size=(3, 3), strides=(1,1), activation=\"relu\", name=\"conv2d_g4_01\")(x)\n",
    "    x = Conv2D(filters=512, kernel_size=(3, 3), strides=(1,1), activation=\"relu\", name=\"conv2d_g4_02\")(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name=\"maxpool_g4\")(x)\n",
    "        \n",
    "    # Group 5:\n",
    "    x = Conv2D(filters=512, kernel_size=(3, 3), strides=(1,1), activation=\"relu\", name=\"conv2d_g5_01\")(x)\n",
    "    x = Conv2D(filters=512, kernel_size=(3, 3), strides=(1,1), activation=\"relu\", name=\"conv2d_g5_02\")(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name=\"maxpool_g5\")(x)\n",
    "    \n",
    "    # Final weight layers:\n",
    "    # Note: The ReLu and Dropout stages here aren't part of original VGG paper, but the website by the authors\n",
    "    # of the paper lists a slightly different version of the paper model that is their \"best\", which \n",
    "    # includes these layers (http://www.robots.ox.ac.uk/~vgg/research/very_deep/): \n",
    "    x = Flatten()(x)\n",
    "    x = Dense(units=4096, name=\"final_fc_1\")(x)\n",
    "    x = Activation(activation='relu')(x)\n",
    "    x = Dropout(rate=0.5)(x)\n",
    "    x = Dense(units=4096, name=\"final_fc_2\")(x)\n",
    "    x = Activation(activation='relu')(x)\n",
    "    x = Dropout(rate=0.5)(x)\n",
    "    \n",
    "    x = Dense(units=num_classes, activation='softmax', name='predictions')(x)\n",
    "#     x = Dense(units=1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
    "    \n",
    "    # VGG paper says \"Finally, to obtain a fixed-size vector of class scores for the image, \n",
    "    # the class score map is spatially averaged (sum-pooled).\"\n",
    "    # Not sure if/where to add that in. \n",
    "    # UDPATE: We probably don't need it. It's not mentioned in their \n",
    "    # website when they define their \"best\" version of their vgg16 and vgg19 architectures. \n",
    "    # x = GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    model = Model(inputs=image_input, outputs=x, name=name)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel_vgg16(image_shape, num_classes, name=\"vgg16\", weights=\"\"):\n",
    "    ## VGG16 architecture: https://arxiv.org/pdf/1409.1556.pdf\n",
    "    ## Also may be able to use VGG-Face pre-trained weights. Not sure if the VGG-Face\n",
    "    ## architecture is the same as the VGG-16. \n",
    "    ## Note: if the architectures don't match, they probably only differ by a small \n",
    "    ## amount, so we can probably create a separate VGG-Face model based on our VGG16\n",
    "    ## and then use the weights from Oxford: http://www.robots.ox.ac.uk/~vgg/software/vgg_face/ \n",
    "    \n",
    "    # This implementation is based on Configuration D from page 3 of 1409.1556.pdf, so 16 weight layers total: \n",
    "    \n",
    "    # Input (image): \n",
    "    # Note, I read somewhere that for tensorflow the order matters for performance, \n",
    "    # so check if this should be (1, img_size, img_size) instead? \n",
    "    image_input = Input(shape=image_shape, name=\"image_input\")\n",
    "    \n",
    "    # Note about pre-trained weights: \n",
    "    # We have to do some potentially different pre-processing depending on which \n",
    "    # pre-trained weights we use (if we use any). For example per-pixel mean-centering: \n",
    "    # https://gist.github.com/ksimonyan/211839e770f7b538e2d8#file-readme-md \n",
    "    \n",
    "    # Group 1: \n",
    "    x = Conv2D(filters=64, kernel_size=(3, 3), strides=(1,1), activation=\"relu\", name=\"conv2d_g1_01\")(image_input)\n",
    "    x = Conv2D(filters=64, kernel_size=(3, 3), strides=(1,1), activation=\"relu\", name=\"conv2d_g1_02\")(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name=\"maxpool_g1\")(x)\n",
    "    \n",
    "    # Group 2: \n",
    "    x = Conv2D(filters=128, kernel_size=(3, 3), strides=(1,1), activation=\"relu\", name=\"conv2d_g2_01\")(x)\n",
    "    x = Conv2D(filters=128, kernel_size=(3, 3), strides=(1,1), activation=\"relu\", name=\"conv2d_g2_02\")(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name=\"maxpool_g2\")(x)\n",
    "        \n",
    "    # Group 3: \n",
    "    x = Conv2D(filters=256, kernel_size=(3, 3), strides=(1,1), activation=\"relu\", name=\"conv2d_g3_01\")(x)\n",
    "    x = Conv2D(filters=256, kernel_size=(3, 3), strides=(1,1), activation=\"relu\", name=\"conv2d_g3_02\")(x)\n",
    "    x = Conv2D(filters=256, kernel_size=(3, 3), strides=(1,1), activation=\"relu\", name=\"conv2d_g3_03\")(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name=\"maxpool_g3\")(x)\n",
    "        \n",
    "    # Group 4: \n",
    "    x = Conv2D(filters=512, kernel_size=(3, 3), strides=(1,1), activation=\"relu\", name=\"conv2d_g4_01\")(x)\n",
    "    x = Conv2D(filters=512, kernel_size=(3, 3), strides=(1,1), activation=\"relu\", name=\"conv2d_g4_02\")(x)\n",
    "    x = Conv2D(filters=512, kernel_size=(3, 3), strides=(1,1), activation=\"relu\", name=\"conv2d_g4_03\")(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name=\"maxpool_g4\")(x)\n",
    "        \n",
    "    # Group 5:\n",
    "    x = Conv2D(filters=512, kernel_size=(3, 3), strides=(1,1), activation=\"relu\", name=\"conv2d_g5_01\")(x)\n",
    "    x = Conv2D(filters=512, kernel_size=(3, 3), strides=(1,1), activation=\"relu\", name=\"conv2d_g5_02\")(x)\n",
    "    x = Conv2D(filters=512, kernel_size=(3, 3), strides=(1,1), activation=\"relu\", name=\"conv2d_g5_03\")(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name=\"maxpool_g5\")(x)\n",
    "    \n",
    "    # Final weight layers:\n",
    "    # Note: The ReLu and Dropout stages here aren't part of original VGG paper, but the website by the authors\n",
    "    # of the paper lists a slightly different version of the paper model that is their \"best\", which \n",
    "    # includes these layers (http://www.robots.ox.ac.uk/~vgg/research/very_deep/): \n",
    "    x = Flatten()(x)\n",
    "    x = Dense(units=4096, name=\"final_fc_1\")(x)\n",
    "    x = Activation(activation='relu')(x)\n",
    "    x = Dropout(rate=0.5)(x)\n",
    "    x = Dense(units=4096, name=\"final_fc_2\")(x)\n",
    "    x = Activation(activation='relu')(x)\n",
    "    x = Dropout(rate=0.5)(x)\n",
    "    \n",
    "    x = Dense(units=num_classes, activation='softmax', name='predictions')(x)\n",
    "#     x = Dense(units=1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
    "    \n",
    "    # VGG paper says \"Finally, to obtain a fixed-size vector of class scores for the image, \n",
    "    # the class score map is spatially averaged (sum-pooled).\"\n",
    "    # Not sure if/where to add that in. \n",
    "    # UDPATE: We probably don't need it. It's not mentioned in their \n",
    "    # website when they define their \"best\" version of their vgg16 and vgg19 architectures. \n",
    "    # x = GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    model = Model(inputs=image_input, outputs=x, name=name)\n",
    "    \n",
    "    if weights != \"\":\n",
    "        weights_dir = \"./weights/\"\n",
    "        weights_path = os.path.join(weights_dir, weights)\n",
    "        model.load_weights(weights_path, by_name=True)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = getModel_vgg16(\n",
    "    image_shape = (image_size, image_size, 3) if color_mode==\"rgb\" else (image_size, image_size, 1)\n",
    "    , num_classes = 2\n",
    "#     , weights = \"vgg16\"\n",
    ")\n",
    "\n",
    "# plot_model(model, show_shapes=True, show_layer_names=False)\n",
    "\n",
    "optimizer = optimizers.SGD(lr=1e-2, decay=5e-4, momentum=0.9, nesterov=True)\n",
    "model.compile(\n",
    "    optimizer=optimizer, \n",
    "    loss='categorical_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_epochs = 74\n",
    "model_name = \"vgg16\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17512 images belonging to 2 classes.\n",
      "Found 1944 images belonging to 2 classes.\n",
      "num woman: 8756\n",
      "num men: 8756\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Get Data batches: \n",
    "train_generator, valid_generator, num_valid, num_samples = getTrainAndTestSet(image_size, batch_size, color_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.\n",
      "Epoch 1/74\n",
      "273/273 [==============================] - 154s 563ms/step - loss: 0.6933 - acc: 0.5015 - val_loss: 0.6925 - val_acc: 0.5005\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.69252, saving model to ./weights/vgg16-Mon_16_Apr_2018_18_53_00.hdf5\n",
      "Epoch 2/74\n",
      "273/273 [==============================] - 147s 540ms/step - loss: 0.6808 - acc: 0.5660 - val_loss: 0.6511 - val_acc: 0.6312\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.69252 to 0.65110, saving model to ./weights/vgg16-Mon_16_Apr_2018_18_53_00.hdf5\n",
      "Epoch 3/74\n",
      "273/273 [==============================] - 150s 548ms/step - loss: 0.6416 - acc: 0.6341 - val_loss: 0.6187 - val_acc: 0.6729\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.65110 to 0.61873, saving model to ./weights/vgg16-Mon_16_Apr_2018_18_53_00.hdf5\n",
      "Epoch 4/74\n",
      "162/273 [================>.............] - ETA: 57s - loss: 0.6155 - acc: 0.6643"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Train: \n",
    "checkpointer = ModelCheckpoint(\n",
    "    filepath = './weights/{}-{}.hdf5'.format(\n",
    "        model_name\n",
    "        , strftime(\"%a_%d_%b_%Y_%H_%M_%S\")\n",
    "    )\n",
    "    , verbose = 1\n",
    "    , save_best_only = True\n",
    ")\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.00000001, verbose=1)\n",
    "callbacks = [reduce_lr, checkpointer]\n",
    "# callbacks = [reduce_lr]\n",
    "\n",
    "history = model.fit_generator(train_generator,\n",
    "                              validation_data=valid_generator,\n",
    "                              validation_steps=num_valid//batch_size,\n",
    "                              steps_per_epoch=num_samples//batch_size, \n",
    "                              epochs=num_epochs,\n",
    "                              callbacks=callbacks,\n",
    "                              verbose=1,\n",
    "                              use_multiprocessing=True,\n",
    "                              workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
